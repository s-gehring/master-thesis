\chapter{Evaluation}
\label{ch:evaluation}
In the following sections, the framework introduced in Chapter~\ref{ch:implementation} will be compared to existing \uima{} distribution approaches. For this reason, a sophisticated evaluation architecture was deployed on a virtual network, which is described in Section~\ref{sec:setup}. The comparison will follow along the metrics given in Section~\ref{sec:requirements}~`\nameref{sec:requirements}'.


Was wir definitiv vergleichen können ist Extensibility und Maintainability, also weiche Metriken. Memory Consumption hab ich nicht mit geloggt, das empfand ich als zuviel Aufwand für zuwenig return. Hintergrund ist, dass sowohl UIMA-AS, als auch Spark nur kleinen Overhead haben. Wir reden hier von <1GB. Jede halbwegs erwachsene Pipeline, mit mindestens einem oder zwei Modellen übertrifft das. Somit bin ich grundsätzlich einfach davon ausgegangen, dass Speicher "genug" da ist, also nichts geswappt wird (und offensichtlich nichts in den OOM-Killer läuft). Ich denke das ist sinnvoll so. Hat man einen Rechner, auf dessen RAM die Pipeline, inklusive aller Modelle, passt, dann passt da auch noch Spark/UIMA-AS drauf. Passt die Pipeline nicht, hat man sowieso Pech.

\section{Setup}
\label{sec:setup}
\marginnote{Daniel fragen was ich für Hardware hatte.}
Setting up a testing and benchmarking environment for the framework given in Chapter~\ref{ch:implementation} and \uimaas{} is not trivial when accounting for comparability. This is because the given framework and \uimaas{} function fundamentally different from each other. The following descriptions for \docker{} architectures suggest a highly similar setup, but even the initialization of both systems differ in terms of concept. While the actual analysis code inside the aggregate \anen{} is provided at the \uimaas{} services inherently, this does not hold for a \spark{} cluster. Such a \spark{} cluster waits for tasks until one arrives and orders it to execute code according to a JAR file, whose \URL{} is given in a parameter. However, the same cluster can work on other problems as well, without reconfiguration and reinitialization. Different tasks can even be processed in parallel, since the \spark{} is not just usable by the frameworks \api{}.

This concept is fundamentally different from \uimaas{}, where a service is bound to exactly one aggregate Analysis Engine and therefore one pipeline. If another \nlp{} algorithm has to be processed, another \uimaas{} service must be instantiated and registered to the broker, at another queue name. While the old services are still up, system resources for those can not be reallocated and idle until a \cas{} object for exactly this \anen{} is sent to the broker. 

These two concepts both have their advantages and disadvantages. The way \spark{} allocates resources for tasks makes it more flexible to use. Different pipelines can easily be processed without making changes to the computation cluster. A cluster of \uimaas{} services would need heavy reconfiguration for such a case. However, since \uimaas{} services do not discard their resources after finishing the current task, a pipeline does not need to be reinitialized if another \cas{} should be analyzed. Since \nlp{} algorithms commonly depend on large dictionaries or language models, this saves on loading time and especially disk I/O. A \spark{} cluster would immediately forget the context of the current pipeline and would have to reinitialize it.

This also comes into play when evaluating the running time of both concepts. Given a more sophisticated pipeline and a ready-to-use \spark{} cluster on one hand and a number of \uimaas{} instances on the other hand, the pipelines in the \uimaas{} services would have already been initialized, while the pipelines in the \spark{} architecture are initialized on-the-fly. This holds not true for \anens{} that load their resources lazily. However, no such assumption can be made in general.

To evaluate both frameworks, and the single threaded approach to get a sense of the administrative workload, a cluster of multiple computers were needed. To simulate a network of machines, \docker{} was chosen as a virtualization concept for multiple reasons. First, \docker{}s resource footprint is way smaller than one of a virtual machine, therefore more resources can be allocated for the actual benchmark. Secondly, a docker image is completely reproducible. While this can also be achieved with configuration management tools like Chef\footurl{https://www.chef.io/chef/}{2018-09-16}, Puppet\footurl{https://puppet.com/de}{2018-09-16} or Ansible\footurl{https://www.ansible.com/}{2018-09-16}, defining a Dockerfile allows for easy reproduction and configuration. 

Notice that network transport delay between the various components are trivial in a simulated network without any artificial delay. For this reason, compression was disabled for the framework presented in this thesis.
\subsection{Apache Spark}
\label{ssec:spark}
In Figure~\ref{fig:arch-spark}, one can see the \docker{} architecture used in the evaluation. A custom created image for \spark{} was used for both, the master and the worker nodes. This image is based on OpenJDK and will be available as described in Section~\ref{sec:availability}. The \spark{} worker nodes can be scaled at will by a simple \docker{} parameter. Both containers, \emph{jar-provider} and \emph{document-provider} are  instances of nginx\footurl{https://www.nginx.com/}{2018-09-16} HTTP servers. The document provider simply provides access to the complete corpus of documents via ordinary HTTP GET requests. These documents could have also been simply copied into the \emph{submitter} image, however an HTTP provider server for the document corpus was chosen to equalize I/O delay both, \uimaas{} and \spark{} would have. This approach is also easily modifiable since the documents provided by the nginx server are a volume, pointing to a persistent folder inside the host file system.
\begin{figure}[tbh]
	\centering
	\input{img/docker-architecture-spark.pdf_tex}
	\caption[The evaluation architecture with Spark inside a Docker environment.]{The evaluation architecture with \spark{} inside a \docker{} environment.}
	\label{fig:arch-spark}
\end{figure}
Similarly, the jar-provider provides the needed JAR files for \spark{} worker nodes. As described above, a \spark{} cluster is a general computation cluster and not pipeline-specific. Thus, to execute code of an analysis engine, the corresponding JAR file must be provided to the whole cluster. While this can also be achieved by mounting a persistent folder inside each worker and the master node, it seemed a cleaner solution to expose the JAR file by another nginx HTTP server. This is closer to real world applications, because a JAR file will most likely not first be copied to each worker nodes file system before executing, especially since it is not trivial to predict which workers are actually processing the given tasks and at what time. In this architecture, any worker can access the JAR file at any point of its lifetime.

The submitter container is also an instance of the \spark{} image described above, but only issues the initial command to the cluster. \spark{} works in one of two modes. First, the standalone mode would let the submitter container also download the JAR file and execute the code until a Java command issues the cluster to work on some tasks in a distributed manner. In cluster mode, a worker node would be allocated by the clusters master to execute the Java code. When a Java command orders \spark{} to parallelize some work, more resources are allocated for said tasks. However, the initial worker node would be unavailable for this phase. Since the architecture is more comparable to the \uimaas{} architecture in the standalone mode, it is chosen for the evaluation.
\subsection{UIMA-AS}
The \docker{} architecture in the evaluation setup for \uimaas{} is very similar to the one for \spark{}. In Figure~\ref{fig:arch-uimaas}, the deployment composition is shown. For the \uimaas{} services, the underlying ApacheMQ broker and the submitter container, an \uimaas{} \docker{} image was composed. As with the \spark{} image described in Section~\ref{ssec:spark}, it is based on an OpenJDK image and will be available to the public according to Section~\ref{sec:availability}.
\begin{figure}[htb]
	\centering
	\input{img/docker-architecture-uimaas.pdf_tex}
	\caption[The evaluation architecture with UIMA-AS inside a Docker environment.]{The evaluation architecture with \uimaas{} inside a \docker{} environment.}
	\label{fig:arch-uimaas}
\end{figure}
First, the \uimaas{} services register themselves at the given broker instance. At the same time the services initialize their corresponding pipeline. This happens in contrast to \spark{}, where the given cluster is completely \uima{} agnostic and initializes the pipelines when they are needed.

In the same way as in Section~\ref{ssec:spark}, the submitter reads the documents per HTTP from the document-provider, which is an instance of the identical image used in the \spark{} evaluation. Notice that even the logic of reading the corpus is identical, since it is wrapped inside a \lstinline|CollectionReader|, which produces \cas{} object that can be processed further. The \cas{} are then sent to the broker and distributed among the services. An important distinction to make is, that in the \uimaas{} concept, the \cas{} object must return to the submitter. In contrast, the concept of the framework built on top of spark does allow the collection of the resulting \cas{}, but discourages it, since it may be a bottleneck on large corpora or large pipelines providing many analysis results.

\subsection{Analysis Engines}
Since the performance is most likely very dependent on the actual analysis to run, or more specifically the given pipeline, three different pipelines were used to compare \uimaas{} and \spark{}:
\begin{itemize}
	\item{}Parsing
	\begin{enumerate}
		\item{}\emph{Language Setter}: Sets the \cas{} language to English. 
		\item{}\emph{Stanford Segmenter \cite{manning-EtAl:2014:P14-5}}: Segments the text into tokens for further processing.
		\item{}\emph{Stanford PoS Tagger \cite{manning-EtAl:2014:P14-5}}: Finds the parts-of-speech for all identified tokens \cite{toutanova2003feature}.
		\item{}\emph{Malt Parser\cite{Nivre05maltparser:a}}: Locates the grammatical components of sentences.
	\end{enumerate}
	\item{}Mixed Named Entity Recognition
	\begin{enumerate}
		\item{}\emph{Language Setter}: Sets the \cas{} language to English. 
		\item{}\emph{OpenNLP Segmenter \cite{opennlp}}: Segments the text into tokens for further processing.
		\item{}\emph{Mate PoS Tagger \cite{bohnet2010very}}: Finds the parts-of-speech for all identified tokens.
		\item{}\emph{ClearNLP Lemmatizer \cite{manning-EtAl:2014:P14-5}}: Generates the base forms for all identified tokens.
		\item{}\emph{Berkeley Parser\cite{petrov2006learning,petrov2007improved}}: Locates the grammatical components of sentences.
		\item{}\emph{Stanford Named Entity Recognizer \cite{manning-EtAl:2014:P14-5}}: Finds occurrences of the entities `Person', `Location', `Organization' and `Misc' and numerical entities.
	\end{enumerate}
	\item{}OpenNLP Named Entity Recognition
	\begin{enumerate}
		\item{}\emph{Language Setter}: Sets the \cas{} language to English. 
		\item{}\emph{OpenNLP Segmenter \cite{opennlp}}: Segments the text into tokens for further processing.
		\item{}\emph{Mate Lemmatizer \cite{bohnet2010very}}: Generates the base forms for all identified tokens.
		\item{}\emph{OpenNLP PoS Tagger \cite{opennlp}}: Finds the parts-of-speech for all identified tokens.
		\item{}\emph{OpenNLP Named Entity Recognizer \cite{opennlp}}: Finds occurrences of entities according to a pre-defined model.
		
	\end{enumerate}
	
\end{itemize}
All the given analysis engines (except for the Language Setter) are provided by the \dkpro{} repository via the build tool maven. 


\section{Results}
The evaluation setup given above was used to analyze a corpus of 3036 text files, taken from the larger dataset of the project Gutenberg\footurl{http://www.gutenberg.org/}{2018-09-16} \cite{lahiri:2014:SRW}. This subset was cleaned by metadata, license information and transcribers notes and were therefore seen as more realistic input data than books containing such artifacts. Furthermore books were chosen as analysis items, because they contain large amount of natural language and may revolve around very different domains. This can be important for the performance of algorithms that solve specific \nlp{} tasks, for example Named Entity Recognition. In the following sections the results of said evaluation are described.

\subsection{CPU Usage}
TBD.
\subsection{RAM Usage}
TBD.
\subsection{Document Throughput}
TBD.
\subsection{Byte Throughput}
TBD.
\subsection{Maintainability}
Given an \anen{} which is used as a pipeline in a \spark{} cluster and an \uimaas{} service collection, changing the underlying Analysis Engine varies from each of the three evaluated approaches. A single threaded native \uima{} instance is exactly as complicated to change as the new Analysis Engine is easy to configure. Since each approach has to deal with this, it can be ignored while comparing all three implementations. This, however, leaves the single threaded approach to be trivial to reconfigure. This is what one can observe in real life applications and stems from the very generic interface the \lstinline|AnalysisEngine| class implements.

Changing \anens{} inside an \uimaas{} environment is substantially harder. Here, all services that serve the specific broker endpoint that is associated with the pipeline, must be shut down and redeployed. This can be automated with tools like \docker{}. This however, would also pose restrictions on log persistence and would add another layer of complexity to the deployment logic. Notice that, without extra tooling such as \docker{}, each service must be shut down separately. There is no centralized command interface.

Switching \anens{} in a \spark{} environment is exactly as easy as the single threaded approach, since \spark{} is \uima{} agnostic. At the moment the \anen{} changes in the original JAR file, every new task \spark{} will pull the changed JAR file and therefore process the given \cas{} objects within the new \anen{} version. Notice that no changes on the cluster must be made and changing the underlying Analysis Engine is possible by only modifying the code in constant time in respect to the cluster size.
\subsection{Implementability}
Since each of the three evaluated implementations, utilizing \spark{}, \uimaas{} or just a single threaded native \uima{} pipeline requires knowledge about the \uima{} environment, the single threaded approach is objectively the easiest to implement. Taking sufficient knowledge of \uima{} as granted, the single threaded approach does not require any more expertise and is therefore trivial. This is not the case with \uimaas{} and the framework presented in Chapter~\ref{ch:implementation}.

\uimaas{} relies heavily on \xml{} files for deployment configuration, which most of must be edited manually. This is an error prone and tedious task when done by hand. The framework Leo, that builds on top of \uimaas{} helps with this by creating \xml{} descriptor files automatically. This however, requires the injection of the Leo framework, another dependency with possible errors and increasing the size of the distributed JAR file. Having dealt with \xml{} descriptor files and the distribution of those, other obstacles occur. Neither log files for debugging purposes nor configuration options of deployed pipeline services are available from outside the service. To make changes to a deployed pipeline, all instances of said \anen{} must be stopped and reinitialized. Log files must be collected by hand, a custom script or by a third party tool. This is opposed to the framework built on top of \spark{}.

Although the framework needs knowledge about the configuration of a given \spark{} cluster, this is done once. While \uimaas{} server instances need to be redeployed whenever a new project starts or changes on \anens{} occur. A \spark{} cluster must be set up once and can then be used indefinitely for different projects and especially \anens{}. In an industry environment, a company wide \spark{} cluster would be imaginable. In such a scenario, configuration of the \spark{} cluster would be done once for the whole company. Thus having knowledge about \spark{} is only partially a requirement, depending on the surrounding environment. Having a running and configured instance of a \spark{} cluster at hand, the execution of the framework becomes trivial as well. Analysis Engines can be changed at will, since \spark{} is completely \uima{} agnostic. Log files are available by either configuring a proper log folder in the initial \spark{} options or by navigating to the \spark{} master node with any browser.
\subsection{Code Quality}
While strictly speaking not a metric measured in the evaluation, code quality is an important factor when deciding upon a scaling framework for \uima{}. Since the frameworks code was optimized according to both static code analyzers FindBugs and SonarLint, it seems unfair to compare their feedback on the framework presented in this thesis with their results on analyzing alternatives like \uimaas{} or Leo. However, it is still a good indicator of code robustness.

While Leo was found to have 29 potential bugs according to FindBugs, and 626 bugs, vulnerabilities and code smells found by SonarLint, \uimaas{} results are way larger. With 3854 bad code artifacts identified by SonarLint and 42 potential bugs shown by FindBugs, its code robustness is debatable. This is especially important on larger projects in a setting where frequent updates are usually not possible, for example in a production environment.

This metric has to be taken with care. As described above, the framework presented in Chapter~\ref{ch:implementation} was specifically designed to output as few finding of both static code analyzers as possible. Also, both analyzers were used with the default configuration, which is usually sensible. However, if one of both, Leo and \uimaas{} would have been programmed with a different set of FindBugs and SonarLint configuration options in mind, many false positives would be expected.

Both, FindBugs and SonarLint do not find any potential bugs or vulnerabilities in the frameworks code. Also the Java compiler used for compiling the framework showed no warnings.

