\chapter{Evaluation}
\label{ch:evaluation}
In the following sections, the framework introduced in Chapter~\ref{ch:implementation} will be compared to existing \uima{} distribution approaches. For this reason, a sophisticated evaluation architecture was deployed on a virtual network, which is described in Section~\ref{sec:setup}. The comparison will follow along the metrics given in Section~\ref{sec:requirements}~`\nameref{sec:requirements}'.


\section{Setup}
\label{sec:setup}

Setting up a testing and benchmarking environment for the framework given in Chapter~\ref{ch:implementation} and \uimaas{} is not trivial when accounting for comparability. This is because the given framework and \uimaas{} function fundamentally different from each other. The following descriptions for \docker{} architectures suggest a highly similar setup, but even the initialization of both systems differ in terms of concept. While the actual analysis code inside the aggregate \anen{} is provided at the \uimaas{} services inherently, this does not hold for a \spark{} cluster. Such a \spark{} cluster waits for tasks until one arrives and orders it to execute code according to a JAR file, whose \URL{} is given in a parameter. However, the same cluster can work on other problems as well, without reconfiguration and reinitialization. Different tasks can even be processed in parallel, since the \spark{} is not just usable by the framework's \api{}.

This concept is fundamentally different from \uimaas{}, where a service is bound to exactly one aggregate Analysis Engine and therefore one pipeline. If another \nlp{} algorithm has to be processed, another \uimaas{} service must be instantiated and registered to the broker, at another queue name. While the old services are still up, system resources for those can not be reallocated and idle until a \cas{} object for exactly this \anen{} is sent to the broker. 

These two concepts both have their advantages and disadvantages. The way \spark{} allocates resources for tasks makes it more flexible to use. Different pipelines can easily be processed without making changes to the computation cluster. A cluster of \uimaas{} services would need heavy reconfiguration for such a case. However, since \uimaas{} services do not discard their resources after finishing the current task, a pipeline does not need to be reinitialized if another \cas{} should be analyzed. Since \nlp{} algorithms commonly depend on large dictionaries or language models, this saves on loading time and especially disk I/O. A \spark{} cluster would immediately forget the context of the current pipeline and would have to reinitialize it.

This also comes into play when evaluating the running time of both concepts. Given a more sophisticated pipeline and a ready-to-use \spark{} cluster on the one hand and a number of \uimaas{} instances on the other, the pipelines in the \uimaas{} services would have already been initialized, while the pipelines in the \spark{} architecture are initialized on-the-fly. This holds not true for \anens{} that load their resources lazily. However, no such assumption can be made in general.

To evaluate both frameworks, and the single-threaded approach to get a sense of the administrative workload, a cluster of multiple computers was needed. To simulate a network of machines, \docker{} was chosen as a virtualization concept for multiple reasons. First, \docker{}s resource footprint is way smaller than one of a virtual machine, therefore more resources can be allocated for the actual benchmark. Secondly, a docker image is completely reproducible. While this can also be achieved with configuration management tools like Chef\footurl{https://www.chef.io/chef/}{2018-09-16}, Puppet\footurl{https://puppet.com/de}{2018-09-16} or Ansible\footurl{https://www.ansible.com/}{2018-09-16}, defining a Dockerfile allows for easy reproduction and configuration. 

Notice that network transport delay between the various components are trivial in a simulated network without any artificial delay. For this reason, compression was disabled for the framework presented in this thesis.

\subsection{Hard- and Software}
The hardware setup consisted of one Intel Xeon E5-1650 with 6x2.6\,GHz with 128\,GB DDR4 memory clocked at 2400\,MHz. The host operating system was an Ubuntu~16.04~LTS with \docker{}~17.12.1-ce installed. For executing the \jvm{}s, openJDK \docker{} images were used, containing the openJDK~8u181. \uima{} and \uimaas{} were used in version 2.10.2 and the \dkpro{} analysis engines are taken from the repository version 1.8.0.

\subsection{Apache Spark}
\label{ssec:spark}
In Figure~\ref{fig:arch-spark}, one can see the \docker{} architecture used in the evaluation. A custom created image for \spark{} was used for both, the master and the worker nodes. This image is based on OpenJDK and will be available as described in Section~\ref{sec:availability}. The \spark{} worker nodes can be scaled at will by a simple \docker{} parameter. Both containers, \emph{jar-provider} and \emph{document-provider} are  instances of nginx\footurl{https://www.nginx.com/}{2018-09-16} HTTP servers. The document provider simply provides access to the complete corpus of documents via ordinary HTTP GET requests. These documents could have also been simply copied into the \emph{submitter} image, however an HTTP provider server for the document corpus was chosen to equalize I/O delay, both \uimaas{} and \spark{} would have. This approach is also easily modifiable since the documents provided by the nginx server are a volume, pointing to a persistent folder inside the host file system.
\begin{figure}[tbh]
	\centering
	\input{img/docker-architecture-spark.pdf_tex}
	\caption[The evaluation architecture with Spark inside a Docker environment.]{The evaluation architecture with \spark{} inside a \docker{} environment.}
	\label{fig:arch-spark}
\end{figure}
Similarly, the jar-provider provides the needed JAR files for \spark{} worker nodes. As described above, a \spark{} cluster is a general computation cluster and not pipeline-specific. Thus, to execute code of an analysis engine, the corresponding JAR file must be provided to the whole cluster. While this can also be achieved by mounting a persistent folder inside each worker and the master node, it seemed a cleaner solution to expose the JAR file by another nginx HTTP server. This is closer to real world applications, because a JAR file will most likely not first be copied to each worker node's file system before executing, especially since it is not trivial to predict which workers are actually processing the given tasks and at what time. In this architecture, any worker can access the JAR file at any point of its lifetime.

The submitter container is also an instance of the \spark{} image described above, but only issues the initial command to the cluster. \spark{} works in one of two modes. First, the standalone mode would let the submitter container also download the JAR file and execute the code until a Java command issues the cluster to work on some tasks in a distributed manner. In cluster mode, a worker node would be allocated by the clusters master to execute the Java code. When a Java command orders \spark{} to parallelize some work, more resources are allocated for said tasks. However, the initial worker node would be unavailable for this phase. Since the architecture is more comparable to the \uimaas{} architecture in the standalone mode, it was chosen for the evaluation.
\subsection{UIMA-AS}
The \docker{} architecture in the evaluation setup for \uimaas{} is very similar to the one for \spark{}. In Figure~\ref{fig:arch-uimaas}, the deployment composition is shown. For the \uimaas{} services, the underlying ApacheMQ broker and the submitter container, an \uimaas{} \docker{} image was composed. As with the \spark{} image described in Section~\ref{ssec:spark}, it is based on an OpenJDK image and will be available to the public according to Section~\ref{sec:availability}.
\begin{figure}[htb]
	\centering
	\input{img/docker-architecture-uimaas.pdf_tex}
	\caption[The evaluation architecture with UIMA-AS inside a Docker environment.]{The evaluation architecture with \uimaas{} inside a \docker{} environment.}
	\label{fig:arch-uimaas}
\end{figure}
First, the \uimaas{} services register themselves at the given broker instance. At the same time the services initialize their corresponding pipeline. This happens in contrast to \spark{}, where the given cluster is completely \uima{} agnostic and initializes the pipelines when they are needed.

In the same way as in Section~\ref{ssec:spark}, the submitter reads the documents per HTTP from the document-provider, which is an instance of the identical image used in the \spark{} evaluation. Notice that even the logic of reading the corpus is identical, since it is wrapped inside a \lstinline|CollectionReader|, which produces \cas{} object that can be processed further. The \cas{} are then sent to the broker and distributed among the services. An important distinction to make is that in the \uimaas{} concept the \cas{} object must return to the submitter. In contrast, the concept of the framework built on top of spark does allow the collection of the resulting \cas{}, but discourages it, since it may be a bottleneck on large corpora or large pipelines providing many analysis results.

\subsection{Analysis Engines}
\label{ssec:pipelines}
Since the performance is most likely very dependent on the actual analysis to run, or more specifically the given pipeline, three different pipelines were used to compare \uimaas{} and \spark{}:
\begin{itemize}
	\item{}Parsing
	\begin{enumerate}
		\item{}\emph{Language Setter}: Sets the \cas{} language to English. 
		\item{}\emph{Stanford Segmenter \cite{manning-EtAl:2014:P14-5}}: Segments the text into tokens for further processing.
		\item{}\emph{Stanford PoS Tagger \cite{manning-EtAl:2014:P14-5}}: Finds the parts-of-speech for all identified tokens \cite{toutanova2003feature}.
		\item{}\emph{Malt Parser\cite{Nivre05maltparser:a}}: Locates the grammatical components of sentences.
	\end{enumerate}
	\item{}Mixed Named Entity Recognition
	\begin{enumerate}
		\item{}\emph{Language Setter}: Sets the \cas{} language to English. 
		\item{}\emph{OpenNLP Segmenter \cite{opennlp}}: Segments the text into tokens for further processing.
		\item{}\emph{Mate PoS Tagger \cite{bohnet2010very}}: Finds the parts-of-speech for all identified tokens.
		\item{}\emph{ClearNLP Lemmatizer \cite{manning-EtAl:2014:P14-5}}: Generates the base forms for all identified tokens.
		\item{}\emph{Berkeley Parser\cite{petrov2006learning,petrov2007improved}}: Locates the grammatical components of sentences.
		\item{}\emph{Stanford Named Entity Recognizer \cite{manning-EtAl:2014:P14-5}}: Finds occurrences of the entities `Person', `Location', `Organization' and `Misc' and numerical entities.
	\end{enumerate}
	\item{}OpenNLP Named Entity Recognition
	\begin{enumerate}
		\item{}\emph{Language Setter}: Sets the \cas{} language to English. 
		\item{}\emph{OpenNLP Segmenter \cite{opennlp}}: Segments the text into tokens for further processing.
		\item{}\emph{Mate Lemmatizer \cite{bohnet2010very}}: Generates the base forms for all identified tokens.
		\item{}\emph{OpenNLP PoS Tagger \cite{opennlp}}: Finds the parts-of-speech for all identified tokens.
		\item{}\emph{OpenNLP Named Entity Recognizer \cite{opennlp}}: Finds occurrences of entities according to a pre-defined model.
		
	\end{enumerate}
	
\end{itemize}
All the given analysis engines (except for the Language Setter) are provided by the \dkpro{} repository via the build tool maven. 

\subsection{Document Corpus}
The evaluation was used to analyze a corpus of 3036 text files, taken from the larger dataset of the project Gutenberg\footurl{http://www.gutenberg.org/}{2018-09-16} \cite{lahiri:2014:SRW}. This subset was cleaned from metadata, license information and transcribers notes and were therefore seen as more realistic input data than books containing such artifacts. Furthermore books were chosen as analysis items, because they contain large amount of natural language and may revolve around very different domains. This can be important for the performance of algorithms that solve specific \nlp{} tasks, for example Named Entity Recognition. 

For the analysis, the corpus was partitioned into three slices of equal cardinality according to file size. The smallest third of all files are files up to a size of 193\,kB. A file is in the middle partition, if it is larger than 193~,kB, but also at most 459\,kB. The largest file was about 9~,MB, therefore an upper limit for even the biggest files is 10000\,kB.
\section{Results}

In the following sections the results of said evaluation are described.

\subsection{CPU Usage}
Evaluating the CPU usage actually turned out to be surprisingly simple. Both, \uimaas{} and the framework implementation of Chapter~\ref{ch:implementation} used all the available processor resources when needed. This is largely due to the fact that every (virtual) machine that represented worker nodes or services respectively, got exactly one processor core. Since no waiting time or disk I/O was necessary to process the pipelines given in Section~\ref{ssec:pipelines}, nothing stopped any of both frameworks utilizing the maximum processor time available.

Both frameworks also acted the same when idling, taking no measurable processor time. This may be due to the blocking system call \lstinline|accept|\footurl{http://man7.org/linux/man-pages/man2/accept.2.html}{2018-09-18} that does not introduce any CPU overhead.
\subsection{RAM Usage}
The usage of RAM was mostly neglected on the evaluation. This is because of two reasons: First, the actually used RAM is highly dependent on the underlying algorithm. It showed that both, \uimaas{} and \spark{} had negligible overhead relative to the memory actually needed by the analysis engines. Even simple \anens{} load models or static resources to process the given \cas{} and if the necessary resources, including the input \cas{} and the resulting analysis objects can not fit into the provided memory, no approach that includes \uima{} is feasible.

The second point why RAM is neglected in this evaluation is the availability of it. The final evaluation run allocated 10\,GB of memory to each processing container. This was an arbitrary choice, as 4\,GB would have also sufficed, which is a low amount of memory in a cluster computation environment and one could argue that a sufficient amount of memory will always be available. Notice that an algorithm does not generally run faster with more available memory. It rather avoids swapping, using the hard drive to extend the available memory, making the actual usage of RAM a binary question: Is there enough to completely avoid swapping? In favor of focusing the other metrics, this question was answered `yes' for this evaluation.

\subsection{Document Throughput}
Analyzing the number of documents that get processed per second shows how fast the different frameworks process the documents without taking account for the differing document sizes. As one can see in Figure~\ref{fig:result:throughput}, the throughput for the single-threaded \uima{} and the \uimaas{} implementation are similar, which seems confusing at first but will be explained in a more detailed view later. It is intuitive that larger document sizes produce smaller results at document throughput. However, \spark{} achieves better results when given more work load in this figure.
\begin{figure}[!htb]
	\centering
	\resizebox{1.\linewidth}{!}{\small\input{img/document_throughput_to_size.pdf_tex}}
	\caption[The document throughput of each framework in the three size categories.]{The document throughput of each framework in the three size categories.}
	\label{fig:result:throughput}
\end{figure}
This is due to the fact that the analysis engine initialization is contained in the deployment of \uimaas{}, but also in the processing time of the \spark{} approach. As discussed earlier, this is a fundamental difference between both frameworks and can not simply be omitted, since initialization is often a costly part of an \anen{} life cycle. However, having larger corpora of data that can be processed at once, without reinitializing the pipelines, this overhead becomes less significant and the document throughput per second rises. This can be seen as a head start for both, the single-threaded approach and \uimaas{}. This head start is sacrificed by \spark{} in order to provide a more flexible interface as explained in Section~\ref{sec:setup}.
\begin{figure}[htb]
	\centering
	\resizebox{1.\linewidth}{!}{\small\input{img/document_throughput_detailed.pdf_tex}}
	\caption{The document throughput of each framework in the three size categories, detailed.}
	\label{fig:result:throughput_det}
\end{figure}
As for the similarity between the single-threaded approach and \uimaas{}, one can see in Figure~\ref{fig:result:throughput_det} that both perform indeed differently, depending on the size of the corpus. Here, the size of the corpus is measured in number of files in the corresponding size category. The single-threaded instance of a native \uima{} pipeline remains stagnant, which is to expect since no increase in administration overhead is necessary to process more data. However, with increasing corpus size, \uimaas{} performs better, since the broker can actually utilize the additional resources.

One can also see an increase in the performance of \spark{}, but it is rather slow compared to \uimaas{}. Only on large corpora of big documents, \spark{} achieves better results than both competitors for reasons described above.

\subsection{Byte Throughput}
Measuring the throughput in terms of bytes per second is a little more complicated than the metric before, because there are different ways to interpret the amount of bytes a process is associated with. Since pipelines usually make annotations and therefore add something to the \cas{} object, the output size is generally not equal to the input size. However, taking compression and serialization into account, the final output size is not equal to the size transmitted between the different components, at least in the case of \spark{} and \uimaas{}.

Thus, two slightly different measures are shown in Figure~\ref{fig:result:throughput_bytes} and Figure~\ref{fig:result:throughput_bytes_after}. Figure~\ref{fig:result:throughput_bytes} shows the number of bytes \emph{before} processing the document and Figure~\ref{fig:result:throughput_bytes_after} shows the same, but calculated with the amount of bytes the \cas{} object holds \emph{after} analysis. Recall that a \cas{} object holds all necessary data, the \sofa{} and all analysis results. Thus it is input and output at the same time, serving well for a size indicator.

In the first figure (Figure~\ref{ref:result:throughput_bytes}), one can see that the single-threaded implementation again stagnates as expected. Since different document sizes are processed exactly the same, the throughput in bytes per second does not change with increasing input sizes. This however, does not hold for \uimaas{} and \spark{}. One can see that \uimaas{} profits from having larger document sizes, most likely because not all available resources could be utilized on the smaller instances. Once every resource is used near optimally, the performance of \uimaas{} also stagnates in terms of byte throughput. Notice that this is to be expected by \emph{any} framework with a constant amount of computation resources.
\begin{figure}[htb]
	\centering
	\resizebox{1.\linewidth}{!}{\small\input{img/byte_throughput_to_size.pdf_tex}}
	\caption{The byte throughput of each framework in the three size categories, measured in bytes before the analysis.}
	\label{fig:result:throughput_bytes}
\end{figure}
\spark{} first drops to a lower performance when increasing the document size and then rises above both other approaches. The first drop is again most likely due to the initialization of analysis engines. For the same reason \uimaas{} performs better at medium input sizes, \spark{} starts to actually use all available resources, triggering the pipeline initialization as often as worker nodes in the \spark{} clusters are. Since the larger documents are bigger by an order of magnitude, this initialization again becomes more and more insignificant.
\begin{figure}[htb]
	\centering
	\resizebox{1.\linewidth}{!}{\small\input{img/byte_throughput_to_size_after.pdf_tex}}
	\caption{The byte throughput of each framework in the three size categories, measured in bytes after the analysis.}
	\label{fig:result:throughput_bytes_after}
\end{figure}
Figure~\ref{fig:result:throughput_bytes_after} shows a similar trend. However, here all algorithms perform worse at smaller input data. This might be because the \cas{} objects do not grow linearly in size after the analysis by the given three pipelines, since fewer annotation types must be shipped within the type system. Another reason is the overhead \uima{} provides itself, making processing of small data more inefficient. However, since \uima{} is the underlying framework for all three approaches, every one of them suffers the consequences.




\subsection{Maintainability}
Given a working \spark{} cluster and the corresponding \uimaas{} service collection, changing the underlying Analysis Engine varies from each of the three evaluated approaches in multiple aspects. 

A single-threaded native \uima{} instance is exactly as complicated to change as the new Analysis Engine is easy to configure. \anens{} can be arbitrarily complex and thus, their configuration options may as well. Since each approach has to deal with this, it can be ignored while comparing all three implementations. This, however, leaves the single-threaded approach to be trivial to reconfigure. One can observe this in real life applications and stems from the very generic interface the \lstinline|AnalysisEngine| class implements.

Changing \anens{} inside an \uimaas{} environment is substantially harder. Here, all services that serve the specific broker endpoint which is associated with the pipeline, must be shut down and redeployed. This can be automated with tools like \docker{}, posing other restrictions on log persistence and adding another layer of complexity to the deployment logic. Notice that, without extra tooling such as \docker{}, each service must be shut down separately. There is no centralized command interface.

Switching \anens{} in a \spark{} environment is exactly as easy as the single-threaded approach, since \spark{} is \uima{} agnostic. At the moment code gets executed that demands the use of a newer version of the \anen{}, \spark{} will pull the modified pipeline code and therefore process the given \cas{} objects within the new \anen{} version. Notice that no changes on the cluster must be made and changing the underlying Analysis Engine is possible by only modifying the code in constant time in respect to the cluster size.
\subsection{Implementability}
Since each of the three evaluated implementations, utilizing \spark{}, \uimaas{} or just a single-threaded native \uima{} pipeline requires knowledge about the \uima{} environment, the single-threaded approach is objectively the easiest to implement. Taking sufficient knowledge of \uima{} as granted, the single-threaded approach does not require any more expertise and is therefore trivial. This is not the case with \uimaas{} and the framework presented in Chapter~\ref{ch:implementation}.

\uimaas{} relies heavily on \xml{} files for deployment configuration, most of which must be edited manually. This is an error-prone and tedious task when done by hand. The framework Leo, that builds on top of \uimaas{} helps with this by creating \xml{} descriptor files automatically. This however, requires the injection of the Leo framework, another dependency with possible errors which also increasing the size of the final code file. Having dealt with \xml{} descriptor files and the distribution of those, other obstacles occur. Neither log files for debugging purposes nor configuration options of deployed pipeline services are available from outside the service. To make changes to a deployed pipeline, all instances of said \anen{} must be stopped and reinitialized. Log files must be collected by hand, a custom script or by a third party tool. This is opposed to the framework built on top of \spark{}.

Although the framework requires knowledge about the configuration of a given \spark{} cluster, this has to be done only once. While \uimaas{} server instances need to be redeployed whenever a new project starts or changes on \anens{} occur. A \spark{} cluster must be set up once and can then be used indefinitely for different projects and especially \anens{}. In an industry environment, a company wide \spark{} cluster would be imaginable. In such a scenario, configuration of the \spark{} cluster would be done once for the whole company. Thus having knowledge about \spark{} is only partially a requirement, depending on the surrounding environment. Having a running and configured instance of a \spark{} cluster at hand, the execution of the framework becomes trivial as well. Analysis Engines can be changed at will, since \spark{} is completely \uima{} agnostic. Log files are available by either configuring a proper log folder in the initial \spark{} options or by navigating to the \spark{} master node with any browser.
\subsection{Code Quality}
While strictly speaking not a metric measured in the evaluation, code quality is an important factor when deciding upon a scaling framework for \uima{}. Since the framework's code was optimized according to both static code analyzers FindBugs and SonarLint, it seems unfair to compare their feedback on the framework presented in this thesis with their results on analyzing alternatives like \uimaas{} or Leo. However, it is still a good indicator of code robustness.

While Leo was found to have 29 potential bugs according to FindBugs, and 626 bugs, vulnerabilities and code smells found by SonarLint, \uimaas{} results are way larger. With 3854 bad code artifacts identified by SonarLint and 42 potential bugs shown by FindBugs, its code robustness is debatable. This is especially important on larger projects in a setting where frequent updates are usually not possible, for example in a production environment.

This metric has to be taken with care. As described above, the framework presented in Chapter~\ref{ch:implementation} was specifically designed to output as few findings of both static code analyzers as possible. Also, both analyzers were used with the default configuration, which is usually sensible. However, if one of both, Leo and \uimaas{} would have been programmed with a different set of FindBugs and SonarLint configuration options in mind, many false positives would be expected.

Both, FindBugs and SonarLint do not find any potential bugs or vulnerabilities in the framework's code. Also the Java compiler used for compiling the framework showed no warnings.

