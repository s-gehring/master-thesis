\chapter{Scaling UIMA with Spark}
\label{ch:implementation}
In this chapter, we will first discuss the choice of \spark{} as a distribution technology. Afterwards the framework implementation details will be documented with a special focus on data distribution, namely serialization and compression.
\section{Selection of Technologies}
In Section~\ref{sec:dist_comp}, two fundamentally different computation models were introduced, namely MapReduce and \rdds{}. Both are generic models of how to parallelize and distribute workload among multiple processing cores. While MapReduce poses more restrictions on the underlying function, it is also more widely used than \rdds{}. Gopalani compared 2015 in \cite{gopalani2015comparing} both methods against each other, choosing \spark{} as the \rdd{} implementation and Hadoop Map Reduce as the implementation for the MapReduce model. For the example case of $K$-Means, he finds that \spark{} performs roughly 50\,\% better in terms of speed. While $K$-Means is a valid choice for an algorithm that can be used in many fields, it is not representative for all parallelizable algorithms. Since this is impossible and \uima{} poses no restrictions on what classes of algorithms can be run inside an \anen{}, the choice of whether to use \rdds{} or a MapReduce implementation is still non-trivial.

Many algorithms in \nlp{} work according to language models, which are usually not only language- but also domain-dependent. General purpose models often do not suffice in terms of domain specific vocabulary and a custom model must be provided \cite{sanderson2010manning}. This is where \spark{} can be used, because it claims to perform better on \emph{iterative tasks} than a MapReduce approach \cite{willmapreduce}. With access to a given language model being a substantial part of many \nlp{} algorithms, \rdds{} seem to suit the \nlp{} needs better than a MapReduce approach. Being the only current implementation of \rdds{}, the choice of distribution framework falls to \spark{}.
\section{Implementation}

The framework presented here consists of several classes that implement different tasks. The framework's main class \lstinline|SharedUimaProcessor| delegates all work to the corresponding classes. One complete execution of the framework, such as an analysis of one corpus of documents, contains several steps to be made. First, the framework must be instantiated. This is done by the actual user. They then order the instance of \lstinline|SharedUimaProcessor| to process a pipeline according to the output of a given collection reader. To accomplish this, the framework has to read the collection, wrap documents into \cas{} objects and send them along with the serialized pipeline description to its workers. After the analysis part is complete, the \cas{} objects get sent back where they are wrapped into a \lstinline|AnalysisResult| object to get access. 
\begin{figure}[!htb]
	\centering
	\resizebox{0.8\linewidth}{!}{\small\input{img/activity-diagram.pdf_tex}}
	\caption[An UML activity diagram of the CAS distribution process.]{An \uml{} activity diagram of the \cas{} distribution process. The executing machine instantiates and distributes the \cas{} objects to the workers, which analyze it in parallel.}
	\label{fig:sup_act}
\end{figure}

Figure~\ref{fig:sup_act} shows the flow of documents in a \uml{} activity diagram. After getting read it is wrapped inside a \cas{} and distributed among worker nodes. There the \cas{} are processed and then sent back. The following sections will describe these steps in detail.

\subsection{Initialization}
\label{sec:init}
The initialization of the framework consists of two parts. First, since it depends on a running \spark{} infrastructure, one of such must be installed. Estimating the performance of algorithms on \spark{} clusters is possible, but non-trivial \cite{wang2015performance,gopalani2015comparing}, especially because it heavily depends on the actual code being processed. Since both, \uima{} and the framework presented here provide the capability to process documents with arbitrary Java code, no assessment can be given at this point. Due to the architecture of the framework the number of usable machines is capped by the number of documents. However, since the corpus to process in a situation when utilizing a scaling framework is necessary is large, this poses no sensible limitation. Another trivial bound is a minimum number of machines, since a single machine would process all \cas{} faster on a native \uima{} instance than a \spark{} cluster containing only one worker could. This is because a \spark{} cluster still has to administrate its only worker. The \cas{} has to be serialized and deserialized twice. The local \uima{} instance skips this.

Given a \spark{} cluster, or more specific, the corresponding Java object \lstinline|JavaSparkContext|, the framework itself must be instantiated. This is useful to process on multiple \spark{} clusters within the same \jvm{}. The class \lstinline|SharedUimaProcessor| provides a constructor 
% Math mode to put everything into one line. Text mode because ttfamily is invalid in math mode ._.
\[ 
\text{\lstinline|SharedUimaProcessor(JavaSparkContext, CompressionAlgorithm, CasSerialization, Logger)|}
\]

While the first parameter \lstinline|JavaSparkContext| was explained above as providing the necessary \api{} to \spark{} for the framework to use, the others have not yet been described. The \lstinline|CompressionAlgorithm| and \lstinline|CasSerialization| parameters are optional and may be \lstinline|null|. They are implementations of interfaces provided by the framework to specify how \cas{} should be serialized and compressed for network transport. This is explained further in Section~\ref{sec:distribution}. The last of the constructor's arguments is an implementation of the popular logging framework interface \lstinline|org.apache.log4j.Logger|.

\subsection{Transport}
Depending on how \spark{} is configured, the user code is either executed directly on the master node (standalone) or on an unrelated machine that sends all necessary parameters to the master node (cluster mode). Usually the standalone mode is chosen only for development or trivial clusters of a single machine, because the underlying call to execute a function is synchronous in such a configuration, therefore the process is not monitorable until the call returns. Figure~\ref{fig:sup_schema} shows the whole process for a cluster mode configuration. Given the initialization described in Section~\ref{sec:init}, a collection reader would read documents into a collection of \cas{} objects. These are then serialized and compressed by algorithms also provided by the user. This is described further in Section~\ref{sec:distribution}. However, after successfully compressing the \cas{}, it gets sent to the \spark{} master node. Notice that this transmission is not necessary in standalone mode, since the \lstinline|SharedUimaProcessor| is then instantiated on the master node itself.
\begin{figure}[htb]
	\centering
	\resizebox{\linewidth}{!}{\small\input{img/shared-uima-processor-schema.pdf_tex}}
	\caption[A schematic for the Shared UIMA Processor in cluster mode.]{A schematic for the Shared \uima{} Processor in cluster mode. The framework is instantiated with parameters, controlling most aspects of it. The \cas{} and the pipeline get sent to the \spark{} cluster, which in turn analyzes the document according to the deployed Analysis Engine.}
	\label{fig:sup_schema}
\end{figure}
Not only the \cas{} are needed to analyze the documents but also the analysis algorithm itself. Analysis Engines, however, are not serializable by Java, since they do not implement the required interface. This is why the framework does not accept instantiated pipelines in form of an aggregate \lstinline|AnalysisEngine|, but only non-instantiated pipelines as \lstinline|AnalysisEngineDescription|, which implements the \lstinline|Serializable| interface. The \lstinline|AnalysisEngineDescription| itself can not be executed but can be used to instantiate the corresponding \anen{}. This happens on all worker nodes simultaneously and is combined with a non-trivial amount of computation time, since Analysis Engines may load large amount of data on the instantiation. Many \nlp{} related algorithms need trained models or dictionaries that are relatively large \cite{sanderson2010manning}. Both, \uima{} and \spark{} provide broadcast read-only variables to load such larger models only once, possibly saving on network and computation resources.

\subsection{Process}
As shown in Figure~\ref{fig:sup_schema}, one can see that a single pipeline is deployed per worker node, or more specifically per \jvm{}. This is important to avoid a limitation of \uima{}s generic nature. Since \anens{} consist of arbitrary code, which can not be guaranteed to be thread-safe in general. To meet the condition not to restrict any \uima{} capabilities, the framework must not pose any restrictions on the Analysis Engines, which includes a guarantee for thread-safety. Instantiating exactly one pipeline per \jvm{} circumvents the problem for the most part, as even static variables accessed by one instance are invisible to other instances. It is to mention that threading issues can still be encountered when accessing external data. The other way such problems may occur, is when deploying an aggregate Analysis Engine containing a delegate \anen{} multiple times. In such a case a custom flow controller could be provided to execute both \anens{} simultaneously. However, this is also a problem in \uima{}s original architecture and can be easily avoided by just using the default Flow Controller or by not adding the same Analysis Engine multiple times in one pipeline.

\subsection{Result}
\begin{figure}[!htb]
	\centering
	\resizebox{\linewidth}{!}{\footnotesize\input{img/class-diagram.pdf_tex}}
	\caption[An UML class diagram of the framework's result classes.]{An \uml{} class diagram of the framework's result classes.}
	\label{fig:sup_results}
\end{figure}
The result type of the framework differs from other framework's like \uimaas{}. The resulting class, \lstinline|AnalysisResult| is very similar to a \lstinline|List<CAS>|, with a few but substantial differences. Figure~\ref{fig:sup_results} shows the \uml{} class diagram of the \lstinline|AnalysisResult| class. Internally it stores a \lstinline|JavaRDD<SerializedCAS>|, which is a class of the \spark{} context. It delegates almost all commands to the underlying \lstinline|JavaRDD| object, however, some functions that are sensible in the \uima{} environment are also provided by this class, for example a \lstinline|saveAsXmi| method, that saves all containing \cas{} objects into a folder. A \lstinline|JavaRDD| behaves much like a \lstinline|List| outside the \spark{} context.


The \lstinline|SerializedCAS| is an internal class that represents a \cas{} that was serialized and compressed with the corresponding algorithms. It simply contains the serialized \lstinline|Byte| array and provides an interface for deserialization by delegating the calls to the corresponding user provided algorithms. It also exposes a \lstinline|size()| function to get the number of bytes needed by the compressed and serialized \cas{}. This is useful for evaluating algorithms that implement the \lstinline|CasSerialization| and \lstinline|CompressionAlgorithm| interfaces. The \lstinline|SerializedCAS| class itself implements the native Java \lstinline|Serializeable| interface and is therefore serializable by the \jvm{}.
% Don't ever touch the magic that happens here!

While \lstinline|JavaRDD<SerializedCAS>| behaves similarly to \lstinline|List<SerializedCAS>|, it is yet fundamentally different in what it does exactly. The native Java \lstinline|Collection| implementations all store data on the local \jvm{} and access them whenever they are needed. However, a \lstinline|JavaRDD| is still a distributed data set among all the worker nodes that provided at least one of the resulting \lstinline|SerializedCAS|. It can now be collected by the \lstinline|AnalysisResult| function \lstinline|collect|. Then all \cas{} objects get sent back to the master node. This is a fundamental difference to \uimaas{}. 

Notice that collecting all analysis results is usually \emph{not} desired when talking about big data collections, because a single machine is likely not able to receive these large amounts of data or store it in a timely matter. Instead, a \cas{} Consumer should be provided at the end of the pipeline. Recall from Section~\ref{ssec:uimacpe} that a \cas{} Consumer is the same as an Analysis Engine in terms of implementation. However, such a \cas{} Consumer would extract the needed analysis results, which are most likely only a sparse subset of all given annotations, and use or store them. This storage is usually done in a database or a distributed file system like \hdfs{} to obtain all results in one place without the need to wait for a single hard drive to write large amounts of data.

\section{Data Distribution}
\label{sec:distribution}
Since all the input data, in form of documents, and output data, in form of analysis results, must be transmitted over a network, be it virtual or real, the serialization of larger Java objects plays a role in performance. Since both, the input and the output, are stored inside a \cas{} object it suffices to find a suitable serialization algorithm for those. However, finding an optimal algorithm is not trivial and usually even depends on the input data. Larger documents produce larger \cas{}, which in turn need a longer time to be deployed to the corresponding \spark{} workers. However, small documents still are no guarantee for small \cas{} sizes, since analysis results can be of arbitrary size and number, depending on the \uima{} pipeline. 
% Don't ever touch the magic that happens here!
\begin{figure}[htb]
	\centering
	\resizebox{\linewidth}{!}{\small\input{img/shared-uima-processor-uml.pdf_tex}}
	\caption[An UML class diagram of the serialization and compression interfaces.]{An \uml{} class diagram of the serialization and compression interfaces.}
	\label{fig:interfaces}
\end{figure}
Furthermore, it can be useful to compress serialized data, depending on the network setup and the serialization algorithm. Most native \uima{} serializations produce \xml{} files, which are very verbose and well compressible. Compression algorithms specifically designed for \xml{} files achieve packing ratios of up to 80\,\% \cite{girardot2005system,min2003xpress,sakr2009xml}. However, such algorithms often come at the price of a relatively high runtime. This is especially undesirable if the transmitted data is small or the serialization sparse and the expected compression ratio is low.

Since an optimal choice for both serialization and compression is not possible for the general case, the framework exposes two interfaces, namely \lstinline|CasSerialization| and \lstinline|CompressionAlgorithm|. Figure~\ref{fig:interfaces} shows the relationship between the framework main class \lstinline|SharedUimaProcessor| and both interfaces. Additionally, two implementations that are already provided by the framework are shown in the model.

\subsection{Serialization}
In \cite{epstein2012making} Epstein et al. explain how serialization of \cas{} was an important bottleneck and a problem to solve. They configured \uimaas{} in several ways to serialize only the parts of the \cas{} object that are needed for further analysis. Obviously this can not be done in the general case when the underlying analysis algorithms are unknown, which is why the framework takes an instance of \lstinline|CasSerialization| as an optional parameter.

An instance of said interface implements two methods with the signatures shown in Listing~\ref{lst:casserialization}.
\begin{lstlisting}[language=Java,caption={CasSerialization method signatures},label=lst:casserialization,float]
public byte[] serialize(CAS cas);
public CAS deserialize(byte[] data, CAS cas);
\end{lstlisting}

While the signature of the \lstinline|serialize| method is intuitive, this does not immediately apply to the \lstinline|deserialize| function. Here, a previously created \cas{} object is given as a parameter for two reasons. First, \uima{} allows for the configuration of a custom \lstinline|CasInitializer|, which can alter the \cas{} object immediately after creation. Although the usage of \lstinline|CasInitializers| has been deprecated since at least 2006, it is still a feature of \uima{} and must therefore be taken care of \cite{uimacpe}. By creating a new \cas{} on the target \jvm{}, the framework first executes the \lstinline|CasInitializers| and then passes the resulting \cas{} to the \lstinline|deserialize| function. The second reason for this additional parameter is to pass the current \uima{} type system. The serialized data might include annotations of types that are unknown to the native \uima{} type system and therefore must be defined before deserialization. Although a parameter \lstinline|TypeSystem| would have sufficed, the first reason implies the requirement of a complete \cas{} parameter. Since the created \cas{} already includes the full type system description, available by \lstinline|cas.getTypeSystem()|, the framework abstains from passing another parameter to the \lstinline|deserialize| method. If \lstinline|CasInitializer|s get removed from \uima{}, this might be a feasible change in the future.

The framework already ships with two implementations of the \lstinline|CasSerialization| interface, namely \lstinline|XmiCasSerialization| and \lstinline|UimaCasSerialization|. The \lstinline|XmiCasSerialization| creates complete \xmi{} files, containing the \sofa{}, all analysis results and even the used type system description. To accomplish this, it uses the \uima{} \lstinline|XmiCasSerializer| class. Thus, the \lstinline|XmiCasSerialization| implementation of \lstinline|CasSerialization| acts as a mere wrapper. The second serialization algorithm \lstinline|UimaCasSerialization| also just wraps around the native \uima{} class \lstinline|Serializer|, which is the same serialization algorithm \uimaas{} uses to distribute and retrieve \cas{} objects. As shown in Figure~\ref{fig:interfaces}, both \lstinline|XmiCasSerialization| and \lstinline|UimaSerialization| are also implementing a singleton pattern, because no instance dependent information must be stored for either of them. However, one could implement a \lstinline|CasSerialization| that stores context dependent information, for example the underlying type system.

\subsection{Compression}
Since the compression results are very dependent on the use case, data size and serialization algorithm, the framework provides the user with a \lstinline|CompressionAlgorithm| interface. An implementation of said interface exposes two methods with signatures as shown in Listing~\ref{lst:cascompression}.
\begin{lstlisting}[language=Java,caption={CompressionAlgorithm method signatures},label=lst:cascompression,float]
public byte[] compress(final byte[] input);
public byte[] decompress(final byte[] input);
\end{lstlisting}
Completely abstracted from any \uima{} concept, this interface simply expects two functions, \lstinline|compress| and \lstinline|decompress| to behave such that for every input \lstinline|byte[] X| holds: 
\[\text{\lstinline|X = decompress(compress(X))|}\]

While this is the only technical requirement for this interface, it is usually desired to have \lstinline+|X| > |compress(X)|+. Since both methods act \uima{} unaware, reducing the object size by omitting parts of the \cas{} is not possible without deserializing the \cas{} first, a step that is defined in the \lstinline|CasSerialization| interface and not accessible from this context. 

The framework ships with two implementations of the \lstinline|CompressionAlgorithm| interface. It defaults to the \lstinline|NoCompression| class, simply implementing the identity with \lstinline|X = compress(X)|, effectively disabling any kind of compression. This is useful if network delay is negligible, especially in virtual networks inside a single machine or on low latency environments. A compression algorithm would need computation time to process all transmitted \cas{}, while saving only a minimum of transfer time. Secondly, the class \lstinline|ZLib| implements the DEFLATE compression, which is a general purpose lossless compression algorithm, commonly used in ZIP files. As seen in Figure~\ref{fig:interfaces} both classes implement the singleton pattern, because no instance data has to be stored for either compression algorithm. However, one could implement an algorithm that stored such information, for example a complete corpus spanning dictionary.

