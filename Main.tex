\documentclass[11pt,a4paper,oneside]{report}

% Packages
\usepackage[utf8]{inputenc}		
\usepackage[T1]{fontenc}			
\usepackage[english]{babel}						% Speling Errors? Me? Never!

\usepackage{amsmath}							% For all your math needs...
\usepackage{amsfonts}							% ...and beyond...
\usepackage{amssymb}							% ...and even further.
\usepackage{makeidx}							% We have a number of indexes.
\usepackage{url}								% URLs in bib
\usepackage{graphicx}							% For pictures and stuff.
\usepackage[pdfusetitle]{hyperref}				% Not really necessary.
\usepackage[acronyms,nohypertypes={acronym,main},nopostdot]{glossaries} % For the glossary.
\usepackage[bottom,perpage,multiple]{footmisc}	% Place footnotes at the bottom of the page, restart counting each page and merge multiple footnotes.
\usepackage{csquotes}							% Quoting. Probably only used in the glossary.
\usepackage[strings]{underscore}				% Bibtex is confused by underscores in URLs. Yeah... underscores.
\usepackage{censor}								% For ████ing that ████ed piece of ████. Actually only for development.
\usepackage{listings}							% Code Snippets.
\usepackage{color}								% Colorful code snippets :3
\usepackage[singlelinecheck=off]{caption}		% Make captions left aligned even if single line.
\usepackage{tocbibind}							% Include ToC in ToC.



% Dev Only
\usepackage{marginnote}							% For development comments.
\renewcommand*{\marginfont}{\tiny}
\usepackage[marginparsep=3mm, marginparwidth=2cm]{geometry}


% Glossary Styling
\setacronymstyle{short-long}
\setglossarystyle{altlist}
\makenoidxglossaries{}
\glstoctrue{}

% Listings Styling
\definecolor{grey}{rgb}{0.8,0.8,0.8}
\lstset{
	tabsize=2,
	rulecolor=\color{black},
	numberstyle=\color{grey},
	numbersep=10pt,
	framesep=5pt,
	xleftmargin=5pt,
	xrightmargin=5pt,
	numbers=left,
	escapeinside={(*@}{@*)},
	keepspaces=true,
	frame=shadowbox,
	captionpos=b,
	breaklines=false,
	basicstyle=\ttfamily\footnotesize,
	aboveskip=20pt,
	belowskip=10pt
}
\lstdefinestyle{YAML}{
	morekeywords={FROM,COPY,ENTRYPOINT,ADD,CMD,HEALTHCHECK,RUN}
}

% Header Imports
\graphicspath{{img/}}
\bibliographystyle{alpha}
\input{definitions}
\input{glossary}

% Meta Data Definitions
\newcommand{\authortext}{Simon Gehring}
\newcommand{\street}{Am Jesuitenhof}
\newcommand{\email}{simon.gehring@fkie.fraunhofer.de}
\newcommand{\housenumber}{3}
\newcommand{\postal}{53117}
\newcommand{\city}{Bonn}
\newcommand{\mn}{2553262}

\newcommand{\doctype}{Master Thesis}
\newcommand{\titletext}{Scaling UIMA}
\newcommand{\subtitletext}{} % Don't know anything interesting.
\newcommand{\field}{Computer Science}
\newcommand{\organization}{Rheinische Friedrich-Wilhelms-Universität Bonn}
\newcommand{\cooperation}{Fraunhofer-Institut für Kommunikation, Informationsverarbeitung und Ergonomie}

\newcommand{\supervisorOne}{Prof.~Dr.~Heiko \textsc{Röglin}}
\newcommand{\supervisorTwo}{Dr.~Timm \textsc{Heuss}}


% Meta Data Compiling
\title{\titletext}
\author{\authortext}
\date{\today}
\newcommand{\address}{\street{} \housenumber{}\linebreak
\postal{} \city{}}

% Document Start
\begin{document}
\input{title}
\pagenumbering{roman}
\tableofcontents{}
\newpage{}
\setcounter{page}{1}
\pagenumbering{arabic}

% Precontent
\begin{abstract}
Natural language is most commonly used to transmit information human-to-human. While most of this interaction takes place orally or written on paper, the digital revolution and the rise of social media increased the amount of digitally stored natural language tremendously. Gantz and Reinsel predicted 2012 that the amount of digital data stored globally will double about every two years until at least the year 2020 \cite{gantz2012digital}.

Many opportunities arise from this amount of digital data, specifically in the field of machine learning. In 2011, IBM's \qa{} system ``Watson'' famously outmatched professional players in the quiz show ``Jeopardy!'' \cite{ferrucci2012introduction,epstein2012making}. Kudesia et al. proposed 2012 an algorithm to detect so called CAUTIs\footnote{Catheter-associated Urinary Tract Infections}, common hospital-acquired infections, by utilizing a \nlp{} analysis with precomputed language models on the medical records of patients \cite{kudesia2012natural}.

Natural language unfortunately tends to be unstructured and hardly machine readable. Even a seemingly easy task, like separating a sentence into words is still an ongoing research topic \cite{pak2018text}. Apache \uima{} is one of few general approaches to implement \nlp{} solutions. With a very modular architecture, \uima{} is a popular tool that can easily be applied to a majority of \nlp{} problems. A large part of the popularity of \uima{} stems from the large \dkpro{} collection of components, containing hundreds of analysis modules and precomputed language models \cite{eckartdecastilho-gurevych:2014:OIAF4HLT}, which are easily imported into existing Java projects with the build automation tool Apache Maven \cite{dkpro}.

A common problem with \uima{} in non-academic environments is scaling \cite{divita2015scaling,epstein2012making,ramakrishnan2010building}. \uima{} itself provides two distinct interfaces to analyze larger collections of unstructured data, with one being \uimaas{} and the other being the more dated and less flexible \cpe{} \cite{OASIS:UIMA:2009}.

In this thesis, we will evaluate different means of scaling \uima{}, using modern technologies like \docker{}, a container virtualization solution, \spark{}, a cluster computing framework, and \kafka{}, an information stream processing software. We will compare said implementations with the native \uimaas{} and \cpe{} approach in terms of processor and memory efficiency, ease of implementation and maintainability. The evaluation will be based on a specific scenario, however it will be easily configurable by exchanging very few lines of code. 

The following decisions are still to be made:
\begin{itemize}
	\item On what level of abstraction will be parallelized? (Inter-Annotator, Inter-Pipeline, Inter-JVM, ...)
	\item What methods of parallelization will be used? (Spark? Native Java Threads? UIMA-AS? Services?)
	\item What application (and implied data set) is going to be parallelized?
	\item What metrics are we trying to optimize?
	\begin{itemize}
		\item CPU Workload (Does the system use all the available processing power?)
		\item RAM Usage (How much RAM does the system use? Does it swap?)
		\item Throughput (How many documents per second are processed?)
		\item Maintainability (Are infrastructure changes or code changes easy to implement?)
		\item Extensibility (How easy is it to add more processing power?)
		\item Implementability (How simple is the implementation?)
	\end{itemize}
\end{itemize}
\end{abstract}

% Content
\input{introduction/main} 		% Einführung, Motivation, Related Work
\input{basics/main}
\input{implementation/main}		% Hauptteil, Lösungen von Problemen, Implementierungsdetail(?)
\input{evaluation/main}			% Evaluation (Geschwindigkeit, Speicher etc.)
\input{summary/main}			% Zusammenfassung / Fazit
\input{future/main}				% Mögliche Future Work


\newpage{}
\pagenumbering{Roman}
\setcounter{page}{1}
% Glossary obv.
\printnoidxglossary[sort=letter]
% Bibliography obv.
\bibliography{bibliography}

% Eidesstattliche Erklärung des Selbstständigen Verfassens
\input{declaration}
  
\end{document}
